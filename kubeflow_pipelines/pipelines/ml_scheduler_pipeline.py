#!/usr/bin/env python3
"""
HYDATIS ML Scheduler - Complete Kubeflow Pipeline
End-to-end MLOps pipeline for automated model training, validation, and deployment.
"""

from kfp.v2 import dsl
from kfp.v2.dsl import pipeline, component, Input, Output, Dataset, Metrics, Model
from kfp.v2.compiler import Compiler
import pandas as pd
import numpy as np
from typing import NamedTuple

# Import custom components
from components.data_validation_component import data_validation_component, feature_engineering_component
from components.model_training_component import xgboost_training_component, qlearning_training_component, isolation_forest_training_component

@component(
    base_image="python:3.9-slim",
    packages_to_install=[
        "mlflow==2.4.1",
        "kubernetes==27.2.0",
        "requests==2.31.0"
    ]
)
def model_validation_component(
    xgboost_model: Input[Model],
    qlearning_model: Input[Model], 
    isolation_forest_model: Input[Model],
    business_targets: dict,
    validation_metrics: Output[Metrics]
) -> NamedTuple("ValidationOutput", [("validation_passed", bool), ("deployment_approved", bool), ("combined_score", float)]):
    """
    Validate all trained models against HYDATIS business requirements.
    
    Args:
        xgboost_model: Trained XGBoost model artifacts
        qlearning_model: Trained Q-Learning model artifacts  
        isolation_forest_model: Trained Isolation Forest model artifacts
        business_targets: HYDATIS business target thresholds
        validation_metrics: Validation performance metrics
        
    Returns:
        ValidationOutput with overall validation status and deployment approval
    """
    import json
    import mlflow
    from datetime import datetime
    from collections import namedtuple
    
    ValidationOutput = namedtuple("ValidationOutput", ["validation_passed", "deployment_approved", "combined_score"])
    
    print("üî¨ Starting comprehensive model validation for HYDATIS")
    
    # Load model artifacts
    with open(xgboost_model.path, 'r') as f:
        xgb_data = json.load(f)
    
    with open(qlearning_model.path, 'r') as f:
        ql_data = json.load(f)
    
    with open(isolation_forest_model.path, 'r') as f:
        if_data = json.load(f)
    
    # Extract model performances
    xgb_cpu_accuracy = xgb_data['models']['cpu_predictor']['accuracy']
    xgb_memory_accuracy = xgb_data['models']['memory_predictor']['accuracy']
    ql_improvement = ql_data['model']['improvement_rate']
    if_precision = if_data['model']['precision']
    if_fpr = if_data['model']['false_positive_rate']
    
    print(f"üìä Model Performance Summary:")
    print(f"   XGBoost CPU Accuracy: {xgb_cpu_accuracy:.4f}")
    print(f"   XGBoost Memory Accuracy: {xgb_memory_accuracy:.4f}")
    print(f"   Q-Learning Improvement: {ql_improvement:.2%}")
    print(f"   Isolation Forest Precision: {if_precision:.4f}")
    print(f"   Isolation Forest FPR: {if_fpr:.4f}")
    
    # Validate against business targets
    validation_results = {
        'xgboost_cpu_target': xgb_cpu_accuracy >= business_targets.get('xgboost_cpu_accuracy', 0.89),
        'xgboost_memory_target': xgb_memory_accuracy >= business_targets.get('xgboost_memory_accuracy', 0.86),
        'qlearning_target': ql_improvement >= business_targets.get('qlearning_improvement', 0.34),
        'isolation_forest_precision_target': if_precision >= business_targets.get('isolation_forest_precision', 0.94),
        'isolation_forest_fpr_target': if_fpr <= business_targets.get('isolation_forest_max_fpr', 0.08)
    }
    
    # Calculate combined performance score
    model_scores = {
        'xgboost': (xgb_cpu_accuracy + xgb_memory_accuracy) / 2,
        'qlearning': min(1.0, ql_improvement),
        'isolation_forest': if_precision * (1 - if_fpr)
    }
    
    combined_score = np.mean(list(model_scores.values()))
    
    # Overall validation
    all_targets_met = all(validation_results.values())
    deployment_approved = all_targets_met and combined_score >= 0.85
    
    print(f"\nüéØ Validation Results:")
    for target, result in validation_results.items():
        print(f"   {target}: {'‚úÖ PASSED' if result else '‚ùå FAILED'}")
    
    print(f"\nüìà Combined Score: {combined_score:.4f}")
    print(f"üöÄ Deployment Approved: {'‚úÖ YES' if deployment_approved else '‚ùå NO'}")
    
    # Log validation metrics
    validation_metrics.log_metric("xgboost_cpu_accuracy", xgb_cpu_accuracy)
    validation_metrics.log_metric("xgboost_memory_accuracy", xgb_memory_accuracy)
    validation_metrics.log_metric("qlearning_improvement", ql_improvement)
    validation_metrics.log_metric("isolation_forest_precision", if_precision)
    validation_metrics.log_metric("isolation_forest_fpr", if_fpr)
    validation_metrics.log_metric("combined_performance_score", combined_score)
    validation_metrics.log_metric("all_targets_met", int(all_targets_met))
    validation_metrics.log_metric("deployment_approved", int(deployment_approved))
    
    # Log individual target achievements
    for target, result in validation_results.items():
        validation_metrics.log_metric(f"target_{target}", int(result))
    
    return ValidationOutput(all_targets_met, deployment_approved, combined_score)


@component(
    base_image="python:3.9-slim",
    packages_to_install=[
        "kubernetes==27.2.0",
        "mlflow==2.4.1",
        "requests==2.31.0"
    ]
)
def model_deployment_component(
    validated_models: Input[Model],
    deployment_config: dict,
    deployment_metrics: Output[Metrics]
) -> NamedTuple("DeploymentOutput", [("deployment_successful", bool), ("model_endpoints_healthy", bool)]):
    """
    Deploy validated models to HYDATIS cluster with progressive rollout.
    
    Args:
        validated_models: Validated model artifacts ready for deployment
        deployment_config: Deployment configuration and environment settings
        deployment_metrics: Deployment performance metrics
        
    Returns:
        DeploymentOutput with deployment status and health validation
    """
    import json
    import time
    import requests
    from kubernetes import client, config
    from datetime import datetime
    from collections import namedtuple
    
    DeploymentOutput = namedtuple("DeploymentOutput", ["deployment_successful", "model_endpoints_healthy"])
    
    print("üöÄ Starting model deployment to HYDATIS cluster")
    
    try:
        # Load Kubernetes configuration
        config.load_incluster_config()  # Running inside cluster
        k8s_apps_v1 = client.AppsV1Api()
        k8s_core_v1 = client.CoreV1Api()
        
    except Exception:
        print("‚ö†Ô∏è Using local kubeconfig for deployment")
        config.load_kube_config()
        k8s_apps_v1 = client.AppsV1Api()
        k8s_core_v1 = client.CoreV1Api()
    
    # Deployment configuration
    namespace = deployment_config.get('namespace', 'hydatis-mlops')
    model_replicas = deployment_config.get('replicas', 3)
    rollout_strategy = deployment_config.get('strategy', 'progressive')
    
    def deploy_model_service(model_name, model_uri, replicas=3):
        """Deploy individual model service to Kubernetes."""
        
        deployment_spec = {
            'apiVersion': 'apps/v1',
            'kind': 'Deployment',
            'metadata': {
                'name': f'{model_name}-predictor',
                'namespace': namespace,
                'labels': {
                    'app': 'ml-scheduler',
                    'component': model_name,
                    'version': 'v1'
                }
            },
            'spec': {
                'replicas': replicas,
                'selector': {
                    'matchLabels': {
                        'app': 'ml-scheduler',
                        'component': model_name
                    }
                },
                'template': {
                    'metadata': {
                        'labels': {
                            'app': 'ml-scheduler',
                            'component': model_name
                        }
                    },
                    'spec': {
                        'containers': [{
                            'name': f'{model_name}-container',
                            'image': 'python:3.9-slim',
                            'ports': [{'containerPort': 8080}],
                            'env': [
                                {'name': 'MODEL_URI', 'value': model_uri},
                                {'name': 'MLFLOW_TRACKING_URI', 'value': 'http://mlflow-server:5000'}
                            ],
                            'resources': {
                                'requests': {'cpu': '500m', 'memory': '1Gi'},
                                'limits': {'cpu': '2', 'memory': '4Gi'}
                            },
                            'livenessProbe': {
                                'httpGet': {'path': '/health', 'port': 8080},
                                'initialDelaySeconds': 30,
                                'periodSeconds': 10
                            },
                            'readinessProbe': {
                                'httpGet': {'path': '/ready', 'port': 8080},
                                'initialDelaySeconds': 15,
                                'periodSeconds': 5
                            }
                        }]
                    }
                }
            }
        }
        
        try:
            # Deploy or update
            k8s_apps_v1.create_namespaced_deployment(
                namespace=namespace,
                body=deployment_spec
            )
            print(f"‚úÖ Deployed {model_name} service")
            return True
            
        except client.ApiException as e:
            if e.status == 409:  # Already exists, update it
                k8s_apps_v1.patch_namespaced_deployment(
                    name=f'{model_name}-predictor',
                    namespace=namespace,
                    body=deployment_spec
                )
                print(f"‚úÖ Updated {model_name} service")
                return True
            else:
                print(f"‚ùå Failed to deploy {model_name}: {e}")
                return False
    
    def create_model_service(model_name):
        """Create Kubernetes service for model endpoint."""
        
        service_spec = {
            'apiVersion': 'v1',
            'kind': 'Service',
            'metadata': {
                'name': f'{model_name}-service',
                'namespace': namespace,
                'labels': {
                    'app': 'ml-scheduler',
                    'component': model_name
                }
            },
            'spec': {
                'selector': {
                    'app': 'ml-scheduler',
                    'component': model_name
                },
                'ports': [{
                    'port': 8080,
                    'targetPort': 8080,
                    'protocol': 'TCP'
                }],
                'type': 'ClusterIP'
            }
        }
        
        try:
            k8s_core_v1.create_namespaced_service(
                namespace=namespace,
                body=service_spec
            )
            print(f"‚úÖ Created {model_name} service")
            return True
            
        except client.ApiException as e:
            if e.status == 409:  # Already exists
                print(f"‚ÑπÔ∏è Service {model_name} already exists")
                return True
            else:
                print(f"‚ùå Failed to create {model_name} service: {e}")
                return False
    
    def validate_deployment_health():
        """Validate all model deployments are healthy."""
        
        model_services = ['xgboost', 'qlearning', 'isolation-forest']
        healthy_services = 0
        
        for service in model_services:
            try:
                # Check deployment status
                deployment = k8s_apps_v1.read_namespaced_deployment(
                    name=f'{service}-predictor',
                    namespace=namespace
                )
                
                ready_replicas = deployment.status.ready_replicas or 0
                desired_replicas = deployment.spec.replicas
                
                if ready_replicas >= desired_replicas:
                    healthy_services += 1
                    print(f"‚úÖ {service} deployment healthy: {ready_replicas}/{desired_replicas} replicas")
                else:
                    print(f"‚ö†Ô∏è {service} deployment not ready: {ready_replicas}/{desired_replicas} replicas")
                
            except Exception as e:
                print(f"‚ùå Failed to check {service} deployment: {e}")
        
        return healthy_services == len(model_services)
    
    # Main deployment logic
    print(f"üéØ Deploying to namespace: {namespace}")
    print(f"üìà Deployment strategy: {rollout_strategy}")
    
    deployment_success = True
    
    # Deploy XGBoost models
    print("üå≤ Deploying XGBoost load predictors...")
    xgb_cpu_deployed = deploy_model_service('xgboost-cpu', xgb_data['models']['cpu_predictor']['model_uri'])
    xgb_memory_deployed = deploy_model_service('xgboost-memory', xgb_data['models']['memory_predictor']['model_uri'])
    
    # Deploy Q-Learning optimizer
    print("üß† Deploying Q-Learning placement optimizer...")
    ql_deployed = deploy_model_service('qlearning', ql_data['model']['model_uri'])
    
    # Deploy Isolation Forest anomaly detector
    print("üîç Deploying Isolation Forest anomaly detector...")
    if_deployed = deploy_model_service('isolation-forest', if_data['model']['model_uri'])
    
    # Create services
    print("üåê Creating Kubernetes services...")
    xgb_service_created = create_model_service('xgboost-cpu') and create_model_service('xgboost-memory')
    ql_service_created = create_model_service('qlearning')
    if_service_created = create_model_service('isolation-forest')
    
    deployment_success = all([
        xgb_cpu_deployed, xgb_memory_deployed, ql_deployed, if_deployed,
        xgb_service_created, ql_service_created, if_service_created
    ])
    
    # Wait for deployments to be ready
    if deployment_success:
        print("‚è≥ Waiting for deployments to be ready...")
        time.sleep(60)  # Wait for pods to start
        
        # Validate deployment health
        endpoints_healthy = validate_deployment_health()
        
        # Log deployment metrics
        deployment_metrics.log_metric("deployment_successful", int(deployment_success))
        deployment_metrics.log_metric("endpoints_healthy", int(endpoints_healthy))
        deployment_metrics.log_metric("xgboost_deployed", int(xgb_cpu_deployed and xgb_memory_deployed))
        deployment_metrics.log_metric("qlearning_deployed", int(ql_deployed))
        deployment_metrics.log_metric("isolation_forest_deployed", int(if_deployed))
        
        print(f"üè• Endpoint health validation: {'‚úÖ HEALTHY' if endpoints_healthy else '‚ùå UNHEALTHY'}")
        
        return ValidationOutput(deployment_success, endpoints_healthy, 1.0 if endpoints_healthy else 0.5)
    else:
        print("‚ùå Deployment failed")
        deployment_metrics.log_metric("deployment_successful", 0)
        deployment_metrics.log_metric("endpoints_healthy", 0)
        
        return ValidationOutput(False, False, 0.0)


@component(
    base_image="python:3.9-slim",
    packages_to_install=[
        "kubernetes==27.2.0",
        "prometheus-api-client==0.5.3",
        "requests==2.31.0"
    ]
)
def business_validation_component(
    deployed_models: Input[Model],
    prometheus_url: str,
    business_targets: dict,
    validation_duration_minutes: int,
    business_metrics: Output[Metrics]
) -> NamedTuple("BusinessValidationOutput", [("cpu_target_met", bool), ("availability_target_met", bool), ("roi_projection", float)]):
    """
    Validate deployed models against HYDATIS business targets in real cluster.
    
    Args:
        deployed_models: Deployed model information
        prometheus_url: Prometheus endpoint for metrics collection
        business_targets: HYDATIS business target thresholds
        validation_duration_minutes: How long to monitor for validation
        business_metrics: Business validation metrics
        
    Returns:
        BusinessValidationOutput with business target achievement status
    """
    import time
    import requests
    import json
    from datetime import datetime, timedelta
    from collections import namedtuple
    
    BusinessValidationOutput = namedtuple("BusinessValidationOutput", ["cpu_target_met", "availability_target_met", "roi_projection"])
    
    print("üíº Starting business validation for HYDATIS targets")
    print(f"‚è±Ô∏è Validation duration: {validation_duration_minutes} minutes")
    
    # Business targets
    cpu_target = business_targets.get('cpu_utilization_target', 0.65)
    availability_target = business_targets.get('availability_target', 0.997)
    roi_target = business_targets.get('roi_target', 14.0)  # 1400%
    
    def collect_business_metrics():
        """Collect current business metrics from HYDATIS cluster."""
        
        # CPU utilization query
        cpu_query = 'avg(100 - (avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100))'
        
        # Availability query (successful pod scheduling rate)
        availability_query = 'avg(rate(scheduler_pod_scheduling_success_total[5m])) / avg(rate(scheduler_pod_scheduling_attempts_total[5m]))'
        
        metrics = {}
        
        try:
            # Query CPU utilization
            cpu_response = requests.get(f"{prometheus_url}/api/v1/query", params={'query': cpu_query}, timeout=10)
            cpu_data = cpu_response.json()
            
            if cpu_data['status'] == 'success' and cpu_data['data']['result']:
                cpu_utilization = float(cpu_data['data']['result'][0]['value'][1]) / 100.0
                metrics['cpu_utilization'] = cpu_utilization
            else:
                metrics['cpu_utilization'] = 0.75  # Conservative fallback
                
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to collect CPU metrics: {e}")
            metrics['cpu_utilization'] = 0.75
        
        try:
            # Query availability
            avail_response = requests.get(f"{prometheus_url}/api/v1/query", params={'query': availability_query}, timeout=10)
            avail_data = avail_response.json()
            
            if avail_data['status'] == 'success' and avail_data['data']['result']:
                availability = float(avail_data['data']['result'][0]['value'][1])
                metrics['availability'] = availability
            else:
                metrics['availability'] = 0.995  # Conservative fallback
                
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to collect availability metrics: {e}")
            metrics['availability'] = 0.995
        
        return metrics
    
    # Monitor business metrics over validation period
    print("üìä Monitoring business metrics...")
    
    validation_start = datetime.now()
    validation_end = validation_start + timedelta(minutes=validation_duration_minutes)
    
    cpu_measurements = []
    availability_measurements = []
    
    while datetime.now() < validation_end:
        metrics = collect_business_metrics()
        cpu_measurements.append(metrics['cpu_utilization'])
        availability_measurements.append(metrics['availability'])
        
        print(f"üìà Current: CPU = {metrics['cpu_utilization']:.1%}, Availability = {metrics['availability']:.3%}")
        
        time.sleep(30)  # Check every 30 seconds
    
    # Calculate validation results
    avg_cpu_utilization = np.mean(cpu_measurements)
    avg_availability = np.mean(availability_measurements)
    
    # Business target validation
    cpu_target_met = avg_cpu_utilization <= cpu_target
    availability_target_met = avg_availability >= availability_target
    
    # ROI projection calculation
    cpu_improvement = max(0, 0.85 - avg_cpu_utilization)  # Improvement from 85% baseline
    availability_improvement = max(0, avg_availability - 0.952)  # Improvement from 95.2% baseline
    
    # ROI calculation based on HYDATIS business case
    infrastructure_savings = cpu_improvement * 30000 * 12  # $30k/month savings per 1% CPU reduction
    availability_savings = availability_improvement * 25000 * 12  # $25k/month per 1% availability improvement
    
    total_annual_savings = infrastructure_savings + availability_savings
    investment_cost = 180000  # $180k development investment
    
    roi_projection = (total_annual_savings / investment_cost) * 100 if investment_cost > 0 else 0
    
    print(f"\nüí∞ Business Validation Results:")
    print(f"   Average CPU Utilization: {avg_cpu_utilization:.1%} (Target: ‚â§{cpu_target:.1%}) {'‚úÖ' if cpu_target_met else '‚ùå'}")
    print(f"   Average Availability: {avg_availability:.3%} (Target: ‚â•{availability_target:.3%}) {'‚úÖ' if availability_target_met else '‚ùå'}")
    print(f"   Projected Annual ROI: {roi_projection:.1f}% (Target: ‚â•{roi_target:.1f}%) {'‚úÖ' if roi_projection >= roi_target else '‚ùå'}")
    
    # Log business metrics
    business_metrics.log_metric("avg_cpu_utilization", avg_cpu_utilization)
    business_metrics.log_metric("avg_availability", avg_availability)
    business_metrics.log_metric("cpu_target_met", int(cpu_target_met))
    business_metrics.log_metric("availability_target_met", int(availability_target_met))
    business_metrics.log_metric("projected_roi", roi_projection)
    business_metrics.log_metric("roi_target_met", int(roi_projection >= roi_target))
    business_metrics.log_metric("validation_duration_minutes", validation_duration_minutes)
    business_metrics.log_metric("cpu_measurements_count", len(cpu_measurements))
    
    return BusinessValidationOutput(cpu_target_met, availability_target_met, roi_projection)


# Main HYDATIS ML Scheduler Pipeline
@pipeline(
    name="hydatis-ml-scheduler-pipeline",
    description="Complete MLOps pipeline for HYDATIS ML-powered Kubernetes scheduler",
    pipeline_root="gs://hydatis-ml-pipeline-artifacts"  # Or s3://hydatis-ml-pipeline-artifacts
)
def hydatis_ml_scheduler_pipeline(
    prometheus_url: str = "http://10.110.190.32:9090",
    mlflow_tracking_uri: str = "http://10.110.190.32:31380",
    data_retention_days: int = 30,
    data_quality_threshold: float = 0.95,
    xgboost_cpu_target: float = 0.89,
    xgboost_memory_target: float = 0.86,
    qlearning_target: float = 0.34,
    isolation_forest_precision: float = 0.94,
    isolation_forest_max_fpr: float = 0.08,
    deployment_namespace: str = "hydatis-mlops",
    business_validation_duration: int = 30
):
    """
    Complete HYDATIS ML Scheduler pipeline with automated training, validation, and deployment.
    
    Pipeline Stages:
    1. Data Validation: Validate cluster data quality and availability
    2. Feature Engineering: Create features for ML model training
    3. Model Training: Train XGBoost, Q-Learning, and Isolation Forest models
    4. Model Validation: Validate models against business targets
    5. Model Deployment: Deploy to HYDATIS cluster with progressive rollout
    6. Business Validation: Validate against CPU, availability, and ROI targets
    """
    
    # Stage 1: Data Validation
    data_validation_task = data_validation_component(
        prometheus_url=prometheus_url,
        data_retention_days=data_retention_days,
        quality_threshold=data_quality_threshold
    )
    
    # Stage 2: Feature Engineering
    feature_engineering_task = feature_engineering_component(
        validated_dataset=data_validation_task.outputs['validated_dataset'],
        feature_store_config={}
    )
    
    # Only proceed if data validation passes
    with dsl.Condition(data_validation_task.outputs['validation_passed'] == True):
        
        # Stage 3: Model Training (Parallel)
        xgboost_training_task = xgboost_training_component(
            engineered_features=feature_engineering_task.outputs['engineered_features'],
            mlflow_tracking_uri=mlflow_tracking_uri,
            target_accuracy=xgboost_cpu_target
        )
        
        qlearning_training_task = qlearning_training_component(
            engineered_features=feature_engineering_task.outputs['engineered_features'],
            mlflow_tracking_uri=mlflow_tracking_uri,
            target_improvement=qlearning_target
        )
        
        isolation_forest_training_task = isolation_forest_training_component(
            engineered_features=feature_engineering_task.outputs['engineered_features'],
            mlflow_tracking_uri=mlflow_tracking_uri,
            target_precision=isolation_forest_precision,
            max_false_positive_rate=isolation_forest_max_fpr
        )
        
        # Stage 4: Model Validation
        model_validation_task = model_validation_component(
            xgboost_model=xgboost_training_task.outputs['trained_model'],
            qlearning_model=qlearning_training_task.outputs['trained_model'],
            isolation_forest_model=isolation_forest_training_task.outputs['trained_model'],
            business_targets={
                'xgboost_cpu_accuracy': xgboost_cpu_target,
                'xgboost_memory_accuracy': xgboost_memory_target,
                'qlearning_improvement': qlearning_target,
                'isolation_forest_precision': isolation_forest_precision,
                'isolation_forest_max_fpr': isolation_forest_max_fpr
            }
        )
        
        # Stage 5: Model Deployment (only if validation passes)
        with dsl.Condition(model_validation_task.outputs['deployment_approved'] == True):
            
            model_deployment_task = model_deployment_component(
                validated_models=model_validation_task.outputs['validation_metrics'],
                deployment_config={
                    'namespace': deployment_namespace,
                    'replicas': 3,
                    'strategy': 'progressive'
                }
            )
            
            # Stage 6: Business Validation (only if deployment successful)
            with dsl.Condition(model_deployment_task.outputs['deployment_successful'] == True):
                
                business_validation_task = business_validation_component(
                    deployed_models=model_deployment_task.outputs['deployment_metrics'],
                    prometheus_url=prometheus_url,
                    business_targets={
                        'cpu_utilization_target': 0.65,
                        'availability_target': 0.997,
                        'roi_target': 14.0
                    },
                    validation_duration_minutes=business_validation_duration
                )


def compile_pipeline():
    """Compile the HYDATIS ML Scheduler pipeline."""
    
    compiler = Compiler()
    compiler.compile(
        pipeline_func=hydatis_ml_scheduler_pipeline,
        package_path="hydatis_ml_scheduler_pipeline.yaml"
    )
    
    print("‚úÖ HYDATIS ML Scheduler pipeline compiled successfully")
    print("üì¶ Pipeline package: hydatis_ml_scheduler_pipeline.yaml")
    
    return "hydatis_ml_scheduler_pipeline.yaml"


def create_recurring_pipeline():
    """Create recurring pipeline for automated retraining."""
    
    recurring_config = {
        'apiVersion': 'argoproj.io/v1alpha1',
        'kind': 'CronWorkflow',
        'metadata': {
            'name': 'hydatis-ml-scheduler-retraining',
            'namespace': 'kubeflow'
        },
        'spec': {
            'schedule': '0 2 * * SUN',  # Weekly retraining on Sundays at 2 AM
            'workflowSpec': {
                'entrypoint': 'hydatis-ml-scheduler-pipeline',
                'templates': [{
                    'name': 'hydatis-ml-scheduler-pipeline',
                    'dag': {
                        'tasks': [{
                            'name': 'run-pipeline',
                            'template': 'pipeline-runner'
                        }]
                    }
                }]
            }
        }
    }
    
    with open('/tmp/recurring_pipeline.yaml', 'w') as f:
        import yaml
        yaml.dump(recurring_config, f)
    
    print("‚úÖ Recurring pipeline configuration created")
    print("üìÖ Schedule: Weekly retraining every Sunday at 2 AM")
    
    return recurring_config


if __name__ == "__main__":
    # Compile pipeline for deployment
    print("üîß Compiling HYDATIS ML Scheduler Pipeline")
    
    pipeline_package = compile_pipeline()
    
    print(f"\nüöÄ Pipeline Ready for Deployment:")
    print(f"   Package: {pipeline_package}")
    print(f"   Components: Data Validation ‚Üí Feature Engineering ‚Üí Model Training ‚Üí Validation ‚Üí Deployment ‚Üí Business Validation")
    print(f"   Target: 10/10 MLOps lifecycle compliance")
    
    # Create recurring pipeline
    recurring_config = create_recurring_pipeline()
    
    print(f"\nüîÑ Automated Retraining Configured:")
    print(f"   Schedule: Weekly (Sundays 2 AM)")
    print(f"   Trigger: Drift detection or scheduled retraining")
    print(f"   Pipeline: Complete end-to-end workflow")