{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Scheduler Data Exploration - Week 1\n",
    "## Historical Cluster Analysis for Intelligent Pod Placement\n",
    "\n",
    "**Objective**: Analyze 30+ days of historical cluster data to identify patterns for ML scheduler development.\n",
    "\n",
    "**Success Criteria**:\n",
    "- Identify temporal patterns in cluster usage\n",
    "- Discover node/workload correlations\n",
    "- Establish baseline performance metrics\n",
    "- Quantify optimization opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Data exploration started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed training data\n",
    "data_path = \"/home/jovyan/data/processed/\"\n",
    "\n",
    "# Find latest processed dataset\n",
    "import os\n",
    "import glob\n",
    "\n",
    "parquet_files = glob.glob(f\"{data_path}ml_scheduler_training_data_*.parquet\")\n",
    "if parquet_files:\n",
    "    latest_file = max(parquet_files, key=os.path.getctime)\n",
    "    df = pd.read_parquet(latest_file)\n",
    "    print(f\"Loaded dataset: {latest_file}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "    print(f\"Nodes: {df['node'].unique()}\")\nelse:\n",
    "    print(\"No processed data found. Run data collection first.\")\n",
    "    df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment\n",
    "if not df.empty:\n",
    "    print(\"=== DATA QUALITY ASSESSMENT ===\")\n",
    "    print(f\"Total records: {len(df):,}\")\n",
    "    print(f\"Features: {len(df.columns)}\")\n",
    "    print(f\"Nodes covered: {len(df['node'].unique())}\")\n",
    "    \n",
    "    # Missing data analysis\n",
    "    missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "    print(\"\\nMissing data by feature:\")\n",
    "    print(missing_pct[missing_pct > 0].sort_values(ascending=False))\n",
    "    \n",
    "    # Temporal coverage\n",
    "    time_diff = df['timestamp'].max() - df['timestamp'].min()\n",
    "    print(f\"\\nTemporal coverage: {time_diff.days} days, {time_diff.seconds//3600} hours\")\n",
    "    \n",
    "    # Records per node\n",
    "    records_per_node = df['node'].value_counts()\n",
    "    print(f\"\\nRecords per node:\")\n",
    "    print(records_per_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal pattern analysis\n",
    "if not df.empty:\n",
    "    print(\"=== TEMPORAL PATTERN ANALYSIS ===\")\n",
    "    \n",
    "    # CPU usage patterns by hour\n",
    "    hourly_cpu = df.groupby('hour')['cpu_usage_rate'].mean()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Hourly patterns\n",
    "    axes[0,0].plot(hourly_cpu.index, hourly_cpu.values, marker='o')\n",
    "    axes[0,0].set_title('Average CPU Usage by Hour')\n",
    "    axes[0,0].set_xlabel('Hour of Day')\n",
    "    axes[0,0].set_ylabel('CPU Usage Rate')\n",
    "    axes[0,0].grid(True)\n",
    "    \n",
    "    # Daily patterns\n",
    "    daily_cpu = df.groupby('day_of_week')['cpu_usage_rate'].mean()\n",
    "    day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    axes[0,1].bar(range(7), daily_cpu.values)\n",
    "    axes[0,1].set_title('Average CPU Usage by Day of Week')\n",
    "    axes[0,1].set_xlabel('Day of Week')\n",
    "    axes[0,1].set_ylabel('CPU Usage Rate')\n",
    "    axes[0,1].set_xticks(range(7))\n",
    "    axes[0,1].set_xticklabels(day_names)\n",
    "    \n",
    "    # Memory usage patterns\n",
    "    hourly_memory = df.groupby('hour')['memory_usage'].mean()\n",
    "    axes[1,0].plot(hourly_memory.index, hourly_memory.values, marker='o', color='orange')\n",
    "    axes[1,0].set_title('Average Memory Usage by Hour')\n",
    "    axes[1,0].set_xlabel('Hour of Day')\n",
    "    axes[1,0].set_ylabel('Memory Usage')\n",
    "    axes[1,0].grid(True)\n",
    "    \n",
    "    # Load patterns\n",
    "    hourly_load = df.groupby('hour')['load_1m'].mean()\n",
    "    axes[1,1].plot(hourly_load.index, hourly_load.values, marker='o', color='green')\n",
    "    axes[1,1].set_title('Average Load by Hour')\n",
    "    axes[1,1].set_xlabel('Hour of Day')\n",
    "    axes[1,1].set_ylabel('Load (1min)')\n",
    "    axes[1,1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node comparison analysis\n",
    "if not df.empty:\n",
    "    print(\"=== NODE PERFORMANCE COMPARISON ===\")\n",
    "    \n",
    "    # Average resource usage by node\n",
    "    node_stats = df.groupby('node').agg({\n",
    "        'cpu_usage_rate': ['mean', 'std', 'max'],\n",
    "        'memory_usage': ['mean', 'std', 'max'],\n",
    "        'load_1m': ['mean', 'std', 'max']\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"Node performance statistics:\")\n",
    "    print(node_stats)\n",
    "    \n",
    "    # Visualize node comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # CPU comparison\n",
    "    node_cpu_mean = df.groupby('node')['cpu_usage_rate'].mean()\n",
    "    axes[0].bar(node_cpu_mean.index, node_cpu_mean.values)\n",
    "    axes[0].set_title('Average CPU Usage by Node')\n",
    "    axes[0].set_ylabel('CPU Usage Rate')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Memory comparison  \n",
    "    node_memory_mean = df.groupby('node')['memory_usage'].mean()\n",
    "    axes[1].bar(node_memory_mean.index, node_memory_mean.values, color='orange')\n",
    "    axes[1].set_title('Average Memory Usage by Node')\n",
    "    axes[1].set_ylabel('Memory Usage')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Load comparison\n",
    "    node_load_mean = df.groupby('node')['load_1m'].mean()\n",
    "    axes[2].bar(node_load_mean.index, node_load_mean.values, color='green')\n",
    "    axes[2].set_title('Average Load by Node')\n",
    "    axes[2].set_ylabel('Load (1min)')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis for feature selection\n",
    "if not df.empty:\n",
    "    print(\"=== CORRELATION ANALYSIS ===\")\n",
    "    \n",
    "    # Select numeric features for correlation\n",
    "    numeric_features = df.select_dtypes(include=[np.number]).columns\n",
    "    correlation_matrix = df[numeric_features].corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, linewidths=0.5, fmt='.2f')\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # High correlation pairs\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.7:\n",
    "                high_corr_pairs.append((\n",
    "                    correlation_matrix.columns[i],\n",
    "                    correlation_matrix.columns[j], \n",
    "                    corr_val\n",
    "                ))\n",
    "    \n",
    "    print(f\"\\nHigh correlation pairs (>0.7):\")\n",
    "    for feat1, feat2, corr in high_corr_pairs:\n",
    "        print(f\"{feat1} <-> {feat2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline performance metrics establishment\n",
    "if not df.empty:\n",
    "    print(\"=== BASELINE PERFORMANCE METRICS ===\")\n",
    "    \n",
    "    # Current cluster utilization metrics\n",
    "    baseline_metrics = {\n",
    "        'avg_cpu_utilization': df['cpu_usage_rate'].mean(),\n",
    "        'max_cpu_utilization': df['cpu_usage_rate'].max(),\n",
    "        'avg_memory_utilization': df['memory_usage'].mean(),\n",
    "        'max_memory_utilization': df['memory_usage'].max(),\n",
    "        'avg_load_1m': df['load_1m'].mean(),\n",
    "        'max_load_1m': df['load_1m'].max(),\n",
    "        'cpu_utilization_std': df['cpu_usage_rate'].std(),\n",
    "        'memory_utilization_std': df['memory_usage'].std()\n",
    "    }\n",
    "    \n",
    "    print(\"Current cluster baseline metrics:\")\n",
    "    for metric, value in baseline_metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Calculate optimization potential\n",
    "    current_avg_cpu = baseline_metrics['avg_cpu_utilization']\n",
    "    target_cpu = 0.65  # 65% target from plan\n",
    "    optimization_potential = (current_avg_cpu - target_cpu) / current_avg_cpu * 100\n",
    "    \n",
    "    print(f\"\\nOptimization Analysis:\")\n",
    "    print(f\"Current average CPU utilization: {current_avg_cpu:.1%}\")\n",
    "    print(f\"Target CPU utilization: {target_cpu:.1%}\")\n",
    "    print(f\"Optimization potential: {optimization_potential:.1f}%\")\n",
    "    \n",
    "    # Log baseline metrics to MLflow\n",
    "    with mlflow.start_run(run_name=\"baseline_analysis\"):\n",
    "        mlflow.log_metrics(baseline_metrics)\n",
    "        mlflow.log_metric(\"optimization_potential_pct\", optimization_potential)\n",
    "        mlflow.log_param(\"analysis_date\", datetime.now().isoformat())\n",
    "        mlflow.log_param(\"data_points\", len(df))\n",
    "        mlflow.log_param(\"time_span_days\", (df['timestamp'].max() - df['timestamp'].min()).days)\n",
    "        \n",
    "    print(\"\\nBaseline metrics logged to MLflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource utilization trends over time\n",
    "if not df.empty:\n",
    "    print(\"=== RESOURCE UTILIZATION TRENDS ===\")\n",
    "    \n",
    "    # Resample to hourly data for trend analysis\n",
    "    df_hourly = df.set_index('timestamp').groupby('node').resample('1H').agg({\n",
    "        'cpu_usage_rate': 'mean',\n",
    "        'memory_usage': 'mean',\n",
    "        'load_1m': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Plot trends\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for node in df['node'].unique():\n",
    "        node_data = df_hourly[df_hourly['node'] == node]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=node_data['timestamp'],\n",
    "            y=node_data['cpu_usage_rate'],\n",
    "            mode='lines',\n",
    "            name=f'{node} CPU',\n",
    "            line=dict(width=1)\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='CPU Usage Trends by Node',\n",
    "        xaxis_title='Time',\n",
    "        yaxis_title='CPU Usage Rate',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Weekly patterns\n",
    "    weekly_patterns = df.groupby(['day_of_week', 'hour']).agg({\n",
    "        'cpu_usage_rate': 'mean',\n",
    "        'memory_usage': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Create heatmap for weekly CPU patterns\n",
    "    cpu_heatmap = weekly_patterns.pivot(index='hour', columns='day_of_week', values='cpu_usage_rate')\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cpu_heatmap, annot=True, fmt='.3f', cmap='YlOrRd')\n",
    "    plt.title('CPU Usage Heatmap: Hour vs Day of Week')\n",
    "    plt.xlabel('Day of Week (0=Monday)')\n",
    "    plt.ylabel('Hour of Day')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analysis results and insights\n",
    "if not df.empty:\n",
    "    print(\"=== ANALYSIS SUMMARY ===\")\n",
    "    \n",
    "    insights = {\n",
    "        'peak_hour': int(hourly_cpu.idxmax()),\n",
    "        'low_hour': int(hourly_cpu.idxmin()),\n",
    "        'peak_cpu_usage': float(hourly_cpu.max()),\n",
    "        'low_cpu_usage': float(hourly_cpu.min()),\n",
    "        'busiest_node': node_cpu_mean.idxmax(),\n",
    "        'least_busy_node': node_cpu_mean.idxmin(),\n",
    "        'cpu_variation_coefficient': float(df['cpu_usage_rate'].std() / df['cpu_usage_rate'].mean()),\n",
    "        'optimization_opportunity_pct': optimization_potential\n",
    "    }\n",
    "    \n",
    "    print(\"Key insights discovered:\")\n",
    "    print(f\"Peak usage hour: {insights['peak_hour']}:00 ({insights['peak_cpu_usage']:.1%} CPU)\")\n",
    "    print(f\"Low usage hour: {insights['low_hour']}:00 ({insights['low_cpu_usage']:.1%} CPU)\")\n",
    "    print(f\"Busiest node: {insights['busiest_node']}\")\n",
    "    print(f\"Least busy node: {insights['least_busy_node']}\")\n",
    "    print(f\"CPU variation coefficient: {insights['cpu_variation_coefficient']:.3f}\")\n",
    "    print(f\"Optimization opportunity: {insights['optimization_opportunity_pct']:.1f}%\")\n",
    "    \n",
    "    # Save insights\n",
    "    import json\n",
    "    insights_path = \"/home/jovyan/data/analysis_insights.json\"\n",
    "    with open(insights_path, 'w') as f:\n",
    "        json.dump(insights, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nInsights saved to: {insights_path}\")\n",
    "    \n",
    "    # Log insights to MLflow\n",
    "    with mlflow.start_run(run_name=\"pattern_analysis\"):\n",
    "        mlflow.log_metrics(insights)\n",
    "        mlflow.log_artifact(insights_path)\n",
    "    \n",
    "    print(\"Analysis completed and logged to MLflow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}