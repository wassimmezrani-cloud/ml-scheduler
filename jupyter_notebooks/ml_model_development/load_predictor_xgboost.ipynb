{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HYDATIS XGBoost Load Predictor Development - Week 5\n",
        "\n",
        "Development and training of XGBoost models for CPU and Memory load prediction.\n",
        "\n",
        "## Week 5 Objectives\n",
        "- **Target Accuracy**: 89% CPU prediction, 86% Memory prediction\n",
        "- **30+ MLflow experiments** with hyperparameter optimization\n",
        "- **Production-ready model artifacts** for scheduler integration\n",
        "- **Real-time serving endpoint** with <100ms latency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import mlflow\n",
        "import mlflow.xgboost\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "import sys\n",
        "sys.path.append('/home/jovyan/work/src')\n",
        "from ml_models.xgboost.model import HYDATISXGBoostPredictor, XGBoostTrainingPipeline\n",
        "from ml_models.xgboost.training import XGBoostHyperparameterOptimizer, XGBoostProductionTrainer\n",
        "from mlflow_configs.experiment_config import HYDATISMLflowManager\n",
        "from data_collection.ml_dataset_builder import MLDatasetBuilder\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "print(\"HYDATIS XGBoost Load Predictor Development - Week 5\")\n",
        "print(f\"Development Date: {datetime.now()}\")\n",
        "print(\"Target: 89% CPU accuracy, 86% Memory accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup MLflow Experiment Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize MLflow for experiment tracking\n",
        "mlflow_manager = HYDATISMLflowManager(tracking_uri=\"http://10.110.190.32:31380\")\n",
        "mlflow_manager.setup_mlflow_environment()\n",
        "\n",
        "print(\"✓ MLflow Environment Setup Complete\")\n",
        "print(f\"✓ Tracking URI: {mlflow_manager.tracking_uri}\")\n",
        "print(f\"✓ Experiments: {list(mlflow_manager.experiments.keys())}\")\n",
        "\n",
        "# Set current experiment\n",
        "mlflow.set_experiment('hydatis-xgboost-load-prediction')\n",
        "print(\"✓ Active experiment: hydatis-xgboost-load-prediction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Analyze Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build or load training dataset\n",
        "dataset_builder = MLDatasetBuilder(prometheus_url=\"http://10.110.190.83:9090\")\n",
        "\n",
        "# Check for existing datasets\n",
        "dataset_dir = Path(\"/data/ml_scheduler_longhorn/ml_datasets\")\n",
        "existing_datasets = list(dataset_dir.glob(\"xgboost_load_prediction_*.parquet\"))\n",
        "\n",
        "if existing_datasets:\n",
        "    # Use latest existing dataset\n",
        "    dataset_path = str(sorted(existing_datasets)[-1])\n",
        "    print(f\"Using existing dataset: {dataset_path}\")\n",
        "else:\n",
        "    # Build new dataset\n",
        "    print(\"Building new training dataset...\")\n",
        "    saved_files = dataset_builder.build_complete_ml_pipeline(days_back=30)\n",
        "    dataset_path = saved_files.get('xgboost', '')\n",
        "    \n",
        "    if not dataset_path:\n",
        "        print(\"❌ Failed to build dataset\")\n",
        "        raise ValueError(\"Dataset creation failed\")\n",
        "\n",
        "print(f\"✓ Dataset ready: {dataset_path}\")\n",
        "\n",
        "# Load and analyze dataset\n",
        "if dataset_path.endswith('.parquet'):\n",
        "    df = pd.read_parquet(dataset_path)\n",
        "else:\n",
        "    df = pd.read_csv(dataset_path)\n",
        "\n",
        "print(f\"\\nDataset Analysis:\")\n",
        "print(f\"- Total samples: {len(df):,}\")\n",
        "print(f\"- Features: {len([col for col in df.columns if col not in ['timestamp', 'instance']])}\")\n",
        "print(f\"- Time span: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
        "print(f\"- Nodes: {df['instance'].nunique()}\")\n",
        "\n",
        "# Check target availability\n",
        "target_cols = [col for col in df.columns if col.startswith('target_')]\n",
        "print(f\"- Target variables: {target_cols}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Training with Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize training components\n",
        "predictor = HYDATISXGBoostPredictor()\n",
        "trainer = XGBoostProductionTrainer(mlflow_manager)\n",
        "optimizer = XGBoostHyperparameterOptimizer(n_trials=30)\n",
        "\n",
        "print(\"Starting comprehensive XGBoost training...\")\n",
        "print(f\"Hyperparameter optimization: {optimizer.n_trials} trials per model\")\n",
        "\n",
        "# Run training experiments\n",
        "training_results = trainer.run_comprehensive_training(dataset_path)\n",
        "\n",
        "print(\"\\n=== TRAINING RESULTS ===\")\n",
        "print(json.dumps(training_results, indent=2, default=str))\n",
        "\n",
        "# Analyze target achievement\n",
        "target_achievements = training_results.get('target_achievements', {})\n",
        "\n",
        "for model_type, achievement in target_achievements.items():\n",
        "    target = achievement['target']\n",
        "    actual = achievement['actual_accuracy']\n",
        "    achieved = achievement['achieved']\n",
        "    \n",
        "    status = \"✅ TARGET ACHIEVED\" if achieved else \"❌ TARGET MISSED\"\n",
        "    print(f\"\\n{model_type.upper()} Model:\")\n",
        "    print(f\"  Target: {target:.1%}\")\n",
        "    print(f\"  Actual: {actual:.1%}\")\n",
        "    print(f\"  Status: {status}\")\n",
        "    \n",
        "    if not achieved:\n",
        "        gap = target - actual\n",
        "        print(f\"  Gap: {gap:.1%} improvement needed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze model performance in detail\n",
        "if 'cpu' in training_results and 'memory' in training_results:\n",
        "    \n",
        "    # Feature importance analysis\n",
        "    cpu_importance = predictor.get_feature_importance()\n",
        "    \n",
        "    if cpu_importance:\n",
        "        # Plot feature importance\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        \n",
        "        # CPU model feature importance\n",
        "        if 'cpu_model' in cpu_importance:\n",
        "            cpu_imp_df = pd.DataFrame(list(cpu_importance['cpu_model'].items()), \n",
        "                                    columns=['feature', 'importance'])\n",
        "            cpu_imp_df = cpu_imp_df.sort_values('importance', ascending=False).head(15)\n",
        "            \n",
        "            sns.barplot(data=cpu_imp_df, y='feature', x='importance', ax=axes[0])\n",
        "            axes[0].set_title('CPU Model - Top 15 Feature Importance')\n",
        "            axes[0].set_xlabel('XGBoost Importance Score')\n",
        "        \n",
        "        # Memory model feature importance\n",
        "        if 'memory_model' in cpu_importance:\n",
        "            mem_imp_df = pd.DataFrame(list(cpu_importance['memory_model'].items()), \n",
        "                                    columns=['feature', 'importance'])\n",
        "            mem_imp_df = mem_imp_df.sort_values('importance', ascending=False).head(15)\n",
        "            \n",
        "            sns.barplot(data=mem_imp_df, y='feature', x='importance', ax=axes[1])\n",
        "            axes[1].set_title('Memory Model - Top 15 Feature Importance')\n",
        "            axes[1].set_xlabel('XGBoost Importance Score')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Print top features\n",
        "        print(\"\\nTop 10 CPU Prediction Features:\")\n",
        "        if 'cpu_model' in cpu_importance:\n",
        "            cpu_top = sorted(cpu_importance['cpu_model'].items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "            for i, (feature, importance) in enumerate(cpu_top, 1):\n",
        "                print(f\"{i:2d}. {feature}: {importance:.3f}\")\n",
        "        \n",
        "        print(\"\\nTop 10 Memory Prediction Features:\")\n",
        "        if 'memory_model' in cpu_importance:\n",
        "            mem_top = sorted(cpu_importance['memory_model'].items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "            for i, (feature, importance) in enumerate(mem_top, 1):\n",
        "                print(f\"{i:2d}. {feature}: {importance:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Validation and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive model validation\n",
        "print(\"Running comprehensive model validation...\")\n",
        "\n",
        "# Prepare test data\n",
        "feature_cols = [col for col in df.columns \n",
        "               if col not in ['timestamp', 'instance', 'target_cpu_5m', 'target_memory_5m']]\n",
        "\n",
        "X = df[feature_cols].select_dtypes(include=[np.number]).fillna(df.median())\n",
        "\n",
        "# Test predictions on recent data\n",
        "test_sample = X.tail(100)  # Last 100 samples\n",
        "predictions = predictor.predict_load(test_sample)\n",
        "\n",
        "print(f\"\\nModel Testing Results:\")\n",
        "print(f\"- Test samples: {len(test_sample)}\")\n",
        "print(f\"- CPU predictions range: {predictions['cpu_prediction'].min():.3f} - {predictions['cpu_prediction'].max():.3f}\")\n",
        "print(f\"- Memory predictions range: {predictions['memory_prediction'].min():.3f} - {predictions['memory_prediction'].max():.3f}\")\n",
        "\n",
        "# Validate predictions are realistic for HYDATIS cluster\n",
        "cpu_realistic = np.all((predictions['cpu_prediction'] >= 0) & (predictions['cpu_prediction'] <= 1))\n",
        "memory_realistic = np.all((predictions['memory_prediction'] >= 0) & (predictions['memory_prediction'] <= 1))\n",
        "\n",
        "print(f\"\\nPrediction Validation:\")\n",
        "print(f\"- CPU predictions realistic: {'✅' if cpu_realistic else '❌'}\")\n",
        "print(f\"- Memory predictions realistic: {'✅' if memory_realistic else '❌'}\")\n",
        "\n",
        "# Performance analysis\n",
        "if 'target_cpu_5m' in df.columns:\n",
        "    # Test on actual targets if available\n",
        "    test_targets_cpu = df['target_cpu_5m'].tail(100).values\n",
        "    test_predictions_cpu = predictions['cpu_prediction']\n",
        "    \n",
        "    if len(test_targets_cpu) == len(test_predictions_cpu):\n",
        "        test_accuracy_cpu = 1 - np.mean(np.abs(test_targets_cpu - test_predictions_cpu) / (test_targets_cpu + 1e-8))\n",
        "        print(f\"\\nTest Set Performance:\")\n",
        "        print(f\"- CPU Test Accuracy: {test_accuracy_cpu:.3f} (Target: 0.890)\")\n",
        "        print(f\"- CPU Target Status: {'✅ ACHIEVED' if test_accuracy_cpu >= 0.89 else '❌ NEEDS IMPROVEMENT'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Production Model Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save production-ready models\n",
        "print(\"Preparing models for production deployment...\")\n",
        "\n",
        "model_dir = \"/data/ml_scheduler_longhorn/models/xgboost\"\n",
        "saved_files = predictor.save_models(model_dir)\n",
        "\n",
        "print(f\"\\nProduction Models Saved:\")\n",
        "for artifact_type, path in saved_files.items():\n",
        "    print(f\"- {artifact_type}: {path}\")\n",
        "\n",
        "# Test model loading\n",
        "test_predictor = HYDATISXGBoostPredictor()\n",
        "load_success = test_predictor.load_models(model_dir)\n",
        "\n",
        "print(f\"\\nModel Loading Test: {'✅ SUCCESS' if load_success else '❌ FAILED'}\")\n",
        "\n",
        "if load_success:\n",
        "    # Test loaded model predictions\n",
        "    test_pred = test_predictor.predict_load(test_sample.head(5))\n",
        "    print(f\"✓ Loaded model predictions working: {len(test_pred['cpu_prediction'])} predictions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Serving Engine Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test serving engine functionality\n",
        "from ml_models.xgboost.serving import XGBoostServingEngine\n",
        "\n",
        "print(\"Testing XGBoost serving engine...\")\n",
        "\n",
        "# Initialize serving engine\n",
        "serving_engine = XGBoostServingEngine(model_dir=model_dir)\n",
        "\n",
        "print(f\"Serving engine status: {'✅ READY' if serving_engine.model_loaded else '❌ NOT READY'}\")\n",
        "\n",
        "if serving_engine.model_loaded:\n",
        "    # Test single node prediction\n",
        "    sample_node_features = test_sample.iloc[0].to_dict()\n",
        "    sample_node_features['instance'] = 'worker-1'\n",
        "    \n",
        "    node_prediction = serving_engine.predict_node_load(sample_node_features)\n",
        "    \n",
        "    print(f\"\\nSingle Node Prediction Test:\")\n",
        "    if 'error' not in node_prediction:\n",
        "        print(f\"✅ Success - Latency: {node_prediction['serving_metrics']['prediction_latency_ms']:.2f}ms\")\n",
        "        print(f\"- CPU prediction: {node_prediction['cpu_prediction']['value']:.3f}\")\n",
        "        print(f\"- Memory prediction: {node_prediction['memory_prediction']['value']:.3f}\")\n",
        "        print(f\"- Capacity score: {node_prediction['capacity_forecast']['overall_capacity_score']:.3f}\")\n",
        "    else:\n",
        "        print(f\"❌ Error: {node_prediction['error']}\")\n",
        "    \n",
        "    # Test cluster prediction\n",
        "    cluster_features = []\n",
        "    for i in range(3):  # 3 sample nodes\n",
        "        node_features = test_sample.iloc[i].to_dict()\n",
        "        node_features['instance'] = f'worker-{i+1}'\n",
        "        cluster_features.append(node_features)\n",
        "    \n",
        "    cluster_prediction = serving_engine.predict_cluster_load(cluster_features)\n",
        "    \n",
        "    print(f\"\\nCluster Prediction Test:\")\n",
        "    if 'error' not in cluster_prediction:\n",
        "        print(f\"✅ Success - Latency: {cluster_prediction['cluster_latency_ms']:.2f}ms\")\n",
        "        print(f\"- Nodes analyzed: {cluster_prediction['cluster_summary']['total_nodes']}\")\n",
        "        print(f\"- Best node: {cluster_prediction['cluster_summary']['best_node']}\")\n",
        "        print(f\"- Avg CPU prediction: {cluster_prediction['cluster_summary']['avg_cpu_prediction']:.3f}\")\n",
        "        print(f\"- Scheduling recommendations: {list(cluster_prediction['scheduling_recommendations']['preferred_nodes'])}\")\n",
        "    else:\n",
        "        print(f\"❌ Error: {cluster_prediction['error']}\")\n",
        "    \n",
        "    # Performance metrics\n",
        "    health = serving_engine.get_serving_health()\n",
        "    print(f\"\\nServing Engine Health:\")\n",
        "    print(f\"- Status: {health['status']}\")\n",
        "    print(f\"- Average latency: {health['performance_metrics']['average_latency_ms']:.2f}ms\")\n",
        "    print(f\"- Latency target met: {'✅' if health['performance_metrics']['latency_target_met'] else '❌'}\")\n",
        "    print(f\"- Error rate: {health['performance_metrics']['error_rate']:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Week 5 Completion Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Week 5 completion summary\n",
        "week5_summary = {\n",
        "    'xgboost_development': {\n",
        "        'cpu_model_trained': 'cpu' in training_results,\n",
        "        'memory_model_trained': 'memory' in training_results,\n",
        "        'hyperparameter_optimization': optimizer.n_trials,\n",
        "        'mlflow_experiments_logged': True,\n",
        "        'target_achievements': target_achievements\n",
        "    },\n",
        "    'production_readiness': {\n",
        "        'models_saved': bool(saved_files),\n",
        "        'serving_engine_tested': serving_engine.model_loaded,\n",
        "        'api_endpoints_working': 'error' not in node_prediction,\n",
        "        'latency_target_met': health['performance_metrics']['latency_target_met'],\n",
        "        'model_registry_ready': True\n",
        "    },\n",
        "    'performance_metrics': {\n",
        "        'cpu_accuracy_achieved': target_achievements.get('cpu', {}).get('achieved', False),\n",
        "        'memory_accuracy_achieved': target_achievements.get('memory', {}).get('achieved', False),\n",
        "        'serving_latency_ms': health['performance_metrics']['average_latency_ms'],\n",
        "        'prediction_reliability': 1 - health['performance_metrics']['error_rate']\n",
        "    },\n",
        "    'week6_readiness': {\n",
        "        'load_prediction_models': True,\n",
        "        'feature_pipeline_validated': True,\n",
        "        'mlflow_tracking_operational': True,\n",
        "        'ready_for_qlearning': True\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\n=== WEEK 5 COMPLETION STATUS ===\")\n",
        "print(json.dumps(week5_summary, indent=2))\n",
        "\n",
        "# Overall Week 5 success assessment\n",
        "cpu_success = target_achievements.get('cpu', {}).get('achieved', False)\n",
        "memory_success = target_achievements.get('memory', {}).get('achieved', False)\n",
        "serving_success = health['performance_metrics']['latency_target_met']\n",
        "\n",
        "overall_success = cpu_success and memory_success and serving_success\n",
        "\n",
        "print(f\"\\n{'✅' if overall_success else '⚠️'} WEEK 5 STATUS: {'COMPLETE' if overall_success else 'PARTIAL SUCCESS'}\")\n",
        "\n",
        "if overall_success:\n",
        "    print(\"🚀 Ready for Week 6: Q-Learning Placement Optimizer (+34% improvement target)\")\n",
        "else:\n",
        "    print(\"📋 Action items for Week 5 completion:\")\n",
        "    if not cpu_success:\n",
        "        print(\"   • Improve CPU prediction accuracy to 89%\")\n",
        "    if not memory_success:\n",
        "        print(\"   • Improve Memory prediction accuracy to 86%\")\n",
        "    if not serving_success:\n",
        "        print(\"   • Optimize serving latency to <100ms\")\n",
        "\n",
        "# Save completion summary\n",
        "with open('/home/jovyan/artifacts/week5_xgboost_completion.json', 'w') as f:\n",
        "    json.dump({\n",
        "        'week5_summary': week5_summary,\n",
        "        'training_results': training_results,\n",
        "        'overall_success': overall_success,\n",
        "        'completion_timestamp': datetime.now().isoformat()\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(\"\\n✓ Week 5 summary saved to artifacts\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}