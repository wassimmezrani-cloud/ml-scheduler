#!/bin/bash

# HYDATIS ML Scheduler - Production Deployment Automation
# Complete production deployment with validation and rollback capabilities

set -euo pipefail

# Colors and formatting
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
NC='\033[0m'

# Configuration
NAMESPACE="ml-scheduler"
DEPLOYMENT_ID="hydatis-$(date +%Y%m%d-%H%M%S)"
BACKUP_DIR="/tmp/ml-scheduler-backup-$DEPLOYMENT_ID"
LOG_FILE="/var/log/ml-scheduler-deployment-$DEPLOYMENT_ID.log"

# Business targets for validation
TARGET_CPU=65.0
TARGET_AVAILABILITY=99.7
TARGET_ROI=1400

log() {
    echo -e "${BLUE}[$(date +'%Y-%m-%d %H:%M:%S')]${NC} $1" | tee -a $LOG_FILE
}

error() {
    echo -e "${RED}[ERROR]${NC} $1" | tee -a $LOG_FILE >&2
}

success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1" | tee -a $LOG_FILE
}

warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1" | tee -a $LOG_FILE
}

section() {
    echo -e "\n${PURPLE}=== $1 ===${NC}" | tee -a $LOG_FILE
}

# Deployment phases
PHASES=(
    "pre_deployment_validation"
    "backup_current_state"
    "deploy_infrastructure"
    "deploy_ml_models"
    "deploy_scheduler_plugin"
    "deploy_advanced_monitoring"
    "progressive_rollout"
    "post_deployment_validation"
    "business_metrics_validation"
    "performance_validation"
)

# Parse command line arguments
PHASE=""
DRY_RUN=false
FORCE=false
ROLLBACK_VERSION=""

while [[ $# -gt 0 ]]; do
    case $1 in
        --phase)
            PHASE="$2"
            shift 2
            ;;
        --dry-run)
            DRY_RUN=true
            shift
            ;;
        --force)
            FORCE=true
            shift
            ;;
        --rollback-to)
            ROLLBACK_VERSION="$2"
            shift 2
            ;;
        --help)
            cat <<EOF
HYDATIS ML Scheduler Production Deployment

Usage: $0 [OPTIONS]

Options:
    --phase PHASE           Execute specific deployment phase
    --dry-run              Validate deployment without applying changes
    --force                Force deployment even if validation fails
    --rollback-to VERSION  Rollback to specific version
    --help                 Show this help message

Phases:
    pre_deployment_validation  - Validate cluster and prerequisites
    backup_current_state      - Backup current configuration and state
    deploy_infrastructure     - Deploy storage, monitoring, and caching
    deploy_ml_models         - Deploy ML model serving infrastructure
    deploy_scheduler_plugin  - Deploy ML scheduler plugin
    deploy_advanced_monitoring - Deploy Week 13 advanced monitoring
    progressive_rollout      - Execute progressive traffic rollout
    post_deployment_validation - Validate deployment success
    business_metrics_validation - Validate business target achievement
    performance_validation  - Validate performance requirements

Example:
    $0                          # Full deployment
    $0 --phase deploy_ml_models # Deploy only ML models
    $0 --dry-run               # Validate without deploying
    $0 --rollback-to v1.2.3    # Rollback to version v1.2.3
EOF
            exit 0
            ;;
        *)
            error "Unknown option: $1"
            exit 1
            ;;
    esac
done

# Validation functions
validate_cluster_health() {
    log "Validating cluster health..."
    
    # Check node readiness
    NOT_READY_NODES=$(kubectl get nodes | grep -v Ready | grep -v NAME | wc -l)
    if [ "$NOT_READY_NODES" -gt 0 ]; then
        error "$NOT_READY_NODES nodes are not ready"
        return 1
    fi
    
    # Check resource availability
    TOTAL_CPU=$(kubectl describe nodes | grep -A1 "Allocatable:" | grep cpu | awk '{sum+=$2} END {print sum}')
    TOTAL_MEMORY=$(kubectl describe nodes | grep -A1 "Allocatable:" | grep memory | awk '{sum+=$2} END {print sum}')
    
    log "Cluster resources: ${TOTAL_CPU} CPU cores, ${TOTAL_MEMORY} memory"
    
    # Check cluster version
    K8S_VERSION=$(kubectl version --short | grep Server | awk '{print $3}')
    log "Kubernetes version: $K8S_VERSION"
    
    success "Cluster health validation passed"
    return 0
}

validate_prerequisites() {
    log "Validating deployment prerequisites..."
    
    # Check required tools
    for tool in kubectl docker python3 helm; do
        if ! command -v $tool &> /dev/null; then
            error "Required tool not found: $tool"
            return 1
        fi
    done
    
    # Check namespace exists
    if ! kubectl get namespace $NAMESPACE &>/dev/null; then
        log "Creating namespace $NAMESPACE"
        kubectl create namespace $NAMESPACE
    fi
    
    # Check storage class
    if ! kubectl get storageclass longhorn &>/dev/null; then
        error "Longhorn storage class not found"
        return 1
    fi
    
    # Check monitoring namespace
    if ! kubectl get namespace monitoring &>/dev/null; then
        warning "Monitoring namespace not found - will create"
        kubectl create namespace monitoring
    fi
    
    success "Prerequisites validation passed"
    return 0
}

validate_business_readiness() {
    log "Validating business readiness..."
    
    # Check current business metrics
    CURRENT_CPU=$(curl -s "http://prometheus:9090/api/v1/query?query=avg(100-(avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m]))*100))" | jq -r '.data.result[0].value[1]' 2>/dev/null || echo "85")
    CURRENT_AVAIL=$(curl -s "http://prometheus:9090/api/v1/query?query=avg(up{job=\"kubernetes-nodes\"})*100" | jq -r '.data.result[0].value[1]' 2>/dev/null || echo "95.2")
    
    log "Current metrics: CPU ${CURRENT_CPU}%, Availability ${CURRENT_AVAIL}%"
    log "Target metrics: CPU ${TARGET_CPU}%, Availability ${TARGET_AVAILABILITY}%"
    
    # Validate improvement potential
    CPU_IMPROVEMENT=$(echo "scale=2; $CURRENT_CPU - $TARGET_CPU" | bc)
    AVAIL_IMPROVEMENT=$(echo "scale=2; $TARGET_AVAILABILITY - $CURRENT_AVAIL" | bc)
    
    if (( $(echo "$CPU_IMPROVEMENT > 0" | bc -l) )); then
        log "CPU optimization potential: ${CPU_IMPROVEMENT}% reduction"
    else
        warning "CPU already at or below target"
    fi
    
    if (( $(echo "$AVAIL_IMPROVEMENT > 0" | bc -l) )); then
        log "Availability improvement potential: ${AVAIL_IMPROVEMENT}%"
    else
        warning "Availability already at or above target"
    fi
    
    success "Business readiness validation passed"
    return 0
}

# Deployment phase functions
pre_deployment_validation() {
    section "PRE-DEPLOYMENT VALIDATION"
    
    validate_cluster_health || return 1
    validate_prerequisites || return 1
    validate_business_readiness || return 1
    
    # Additional production-specific checks
    log "Checking production-specific requirements..."
    
    # Check backup systems
    if ! kubectl get pods -n backup | grep -q Running; then
        warning "Backup system not running - deploying backup infrastructure"
        kubectl apply -f k8s_configs/backup/
    fi
    
    # Check monitoring stack
    if ! kubectl get pods -n monitoring | grep -q prometheus; then
        error "Prometheus not running - monitoring required for production"
        return 1
    fi
    
    # Validate network policies
    if ! kubectl get networkpolicies -n $NAMESPACE | grep -q ml-scheduler; then
        log "Applying network security policies"
        kubectl apply -f k8s_configs/security/network-policies.yaml
    fi
    
    success "Pre-deployment validation completed"
}

backup_current_state() {
    section "BACKING UP CURRENT STATE"
    
    mkdir -p $BACKUP_DIR
    
    log "Backing up current configuration to $BACKUP_DIR..."
    
    # Backup configurations
    kubectl get configmaps -n $NAMESPACE -o yaml > $BACKUP_DIR/configmaps.yaml
    kubectl get secrets -n $NAMESPACE -o yaml > $BACKUP_DIR/secrets.yaml
    kubectl get deployments -n $NAMESPACE -o yaml > $BACKUP_DIR/deployments.yaml
    kubectl get services -n $NAMESPACE -o yaml > $BACKUP_DIR/services.yaml
    kubectl get inferenceservices -n $NAMESPACE -o yaml > $BACKUP_DIR/inferenceservices.yaml 2>/dev/null || true
    
    # Backup current metrics (baseline)
    curl -s "http://prometheus:9090/api/v1/query_range?query=avg(100-(avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m]))*100))&start=$(date -d '1 hour ago' --iso-8601)&end=$(date --iso-8601)&step=5m" > $BACKUP_DIR/baseline-cpu-metrics.json
    curl -s "http://prometheus:9090/api/v1/query_range?query=avg(up{job=\"kubernetes-nodes\"})*100&start=$(date -d '1 hour ago' --iso-8601)&end=$(date --iso-8601)&step=5m" > $BACKUP_DIR/baseline-availability-metrics.json
    
    # Backup ML model versions
    curl -s "http://mlflow:5000/api/2.0/mlflow/registered-models/list" > $BACKUP_DIR/model-versions.json 2>/dev/null || echo "{}" > $BACKUP_DIR/model-versions.json
    
    # Create deployment manifest
    cat <<EOF > $BACKUP_DIR/deployment-manifest.yaml
deployment_id: $DEPLOYMENT_ID
timestamp: $(date --iso-8601)
git_commit: ${GITHUB_SHA:-$(git rev-parse HEAD 2>/dev/null || echo "unknown")}
namespace: $NAMESPACE
backup_location: $BACKUP_DIR
baseline_metrics:
  cpu_utilization: $(curl -s "http://prometheus:9090/api/v1/query?query=avg(100-(avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m]))*100))" | jq -r '.data.result[0].value[1]' 2>/dev/null || echo "85")
  availability: $(curl -s "http://prometheus:9090/api/v1/query?query=avg(up{job=\"kubernetes-nodes\"})*100" | jq -r '.data.result[0].value[1]' 2>/dev/null || echo "95.2")
EOF
    
    success "Current state backed up to $BACKUP_DIR"
}

deploy_infrastructure() {
    section "DEPLOYING INFRASTRUCTURE"
    
    if [ "$DRY_RUN" = true ]; then
        log "DRY RUN: Would deploy infrastructure components"
        return 0
    fi
    
    # Deploy storage infrastructure
    log "Deploying storage infrastructure..."
    kubectl apply -f k8s_configs/storage/
    kubectl wait --for=condition=available --timeout=300s deployment/longhorn-manager -n longhorn-system || true
    
    # Deploy Redis cache
    log "Deploying Redis cache..."
    kubectl apply -f k8s_configs/ml_services/redis-deployment.yaml
    kubectl wait --for=condition=available --timeout=180s deployment/redis -n $NAMESPACE
    
    # Validate Redis connectivity
    kubectl run redis-test --rm -i --restart=Never --image=redis:7 -- \
        redis-cli -h redis.$NAMESPACE ping | grep -q PONG || {
        error "Redis connectivity test failed"
        return 1
    }
    
    # Deploy monitoring infrastructure
    log "Deploying monitoring infrastructure..."
    kubectl apply -f k8s_configs/monitoring/
    kubectl wait --for=condition=available --timeout=300s deployment/prometheus -n monitoring
    
    success "Infrastructure deployment completed"
}

deploy_ml_models() {
    section "DEPLOYING ML MODELS"
    
    if [ "$DRY_RUN" = true ]; then
        log "DRY RUN: Would deploy ML model services"
        return 0
    fi
    
    # Deploy KServe serving runtime
    log "Deploying KServe serving runtime..."
    kubectl apply -f kserve_configs/serving-runtime.yaml
    
    # Deploy ML model services
    log "Deploying XGBoost predictor..."
    kubectl apply -f kserve_configs/xgboost-isvc.yaml
    
    log "Deploying Q-Learning optimizer..."
    kubectl apply -f kserve_configs/qlearning-isvc.yaml
    
    log "Deploying Anomaly detector..."
    kubectl apply -f kserve_configs/anomaly-isvc.yaml
    
    # Wait for model services to be ready
    log "Waiting for model services to be ready..."
    for model in xgboost-predictor qlearning-optimizer anomaly-detector; do
        kubectl wait --for=condition=Ready --timeout=300s inferenceservice/$model -n $NAMESPACE || {
            error "Model service $model failed to become ready"
            return 1
        }
        success "$model service ready"
    done
    
    # Validate model endpoints
    log "Validating model endpoints..."
    python scripts/validation/test_model_endpoints.py --all-models --timeout 30 || {
        error "Model endpoint validation failed"
        return 1
    }
    
    success "ML model deployment completed"
}

deploy_scheduler_plugin() {
    section "DEPLOYING ML SCHEDULER PLUGIN"
    
    if [ "$DRY_RUN" = true ]; then
        log "DRY RUN: Would deploy scheduler plugin"
        return 0
    fi
    
    # Apply scheduler configuration
    log "Applying scheduler configuration..."
    kubectl apply -f config/deployment_config.yaml
    
    # Deploy scheduler plugin
    log "Deploying ML scheduler plugin..."
    kubectl apply -f scheduler-plugin/manifests/scheduler-deployment.yaml
    
    # Wait for scheduler to be ready
    kubectl wait --for=condition=available --timeout=300s deployment/ml-scheduler -n $NAMESPACE || {
        error "ML scheduler failed to become ready"
        return 1
    }
    
    # Validate scheduler registration
    log "Validating scheduler registration..."
    sleep 30  # Allow time for registration
    
    if kubectl get leases -n kube-system | grep -q ml-scheduler; then
        success "ML scheduler successfully registered"
    else
        error "ML scheduler registration failed"
        return 1
    fi
    
    success "ML scheduler plugin deployment completed"
}

deploy_advanced_monitoring() {
    section "DEPLOYING ADVANCED MONITORING (WEEK 13)"
    
    if [ "$DRY_RUN" = true ]; then
        log "DRY RUN: Would deploy advanced monitoring"
        return 0
    fi
    
    # Execute Week 13 deployment script
    log "Executing Week 13 advanced monitoring deployment..."
    ./scripts/deploy_week13_services.sh || {
        error "Week 13 deployment failed"
        return 1
    }
    
    # Validate advanced monitoring services
    log "Validating advanced monitoring services..."
    for service in drift-detection-service predictive-analytics-service aiops-remediation-service advanced-anomaly-detection; do
        kubectl wait --for=condition=available --timeout=180s deployment/$service -n $NAMESPACE || {
            error "Advanced monitoring service $service failed to deploy"
            return 1
        }
    done
    
    # Test AIOps webhook
    log "Testing AIOps webhook integration..."
    curl -X POST http://aiops-remediation-service.$NAMESPACE:8080/webhook \
        -H "Content-Type: application/json" \
        -d '{"alerts":[{"alertname":"DeploymentTest","status":"firing","severity":"info"}]}' \
        --max-time 10 || {
        warning "AIOps webhook test failed - may impact automated remediation"
    }
    
    success "Advanced monitoring deployment completed"
}

progressive_rollout() {
    section "EXECUTING PROGRESSIVE ROLLOUT"
    
    if [ "$DRY_RUN" = true ]; then
        log "DRY RUN: Would execute progressive rollout"
        return 0
    fi
    
    log "Starting progressive rollout: 10% ‚Üí 50% ‚Üí 100% traffic"
    
    # Phase 1: 10% traffic
    log "Phase 1: Routing 10% traffic to ML scheduler..."
    python src/deployment/progressive_rollout.py \
        --rollout-id $DEPLOYMENT_ID \
        --phase canary \
        --traffic-percent 10 \
        --validation-window 15 \
        --auto-rollback-on-failure || {
        error "Phase 1 rollout failed"
        return 1
    }
    
    # Validate Phase 1
    sleep 900  # 15 minutes validation window
    if ! validate_phase_metrics 10; then
        error "Phase 1 validation failed - initiating rollback"
        rollback_deployment
        return 1
    fi
    success "Phase 1 (10% traffic) completed successfully"
    
    # Phase 2: 50% traffic
    log "Phase 2: Routing 50% traffic to ML scheduler..."
    python src/deployment/progressive_rollout.py \
        --rollout-id $DEPLOYMENT_ID \
        --phase main \
        --traffic-percent 50 \
        --validation-window 15 \
        --auto-rollback-on-failure || {
        error "Phase 2 rollout failed"
        return 1
    }
    
    # Validate Phase 2
    sleep 900  # 15 minutes validation window
    if ! validate_phase_metrics 50; then
        error "Phase 2 validation failed - initiating rollback"
        rollback_deployment
        return 1
    fi
    success "Phase 2 (50% traffic) completed successfully"
    
    # Phase 3: 100% traffic
    log "Phase 3: Routing 100% traffic to ML scheduler..."
    python src/deployment/progressive_rollout.py \
        --rollout-id $DEPLOYMENT_ID \
        --phase full \
        --traffic-percent 100 \
        --validation-window 30 \
        --auto-rollback-on-failure || {
        error "Phase 3 rollout failed"
        return 1
    }
    
    # Final validation
    sleep 1800  # 30 minutes validation window
    if ! validate_phase_metrics 100; then
        error "Phase 3 validation failed - initiating rollback"
        rollback_deployment
        return 1
    fi
    
    success "Progressive rollout completed successfully"
}

validate_phase_metrics() {
    local traffic_percent=$1
    log "Validating metrics for $traffic_percent% traffic phase..."
    
    # Collect current metrics
    CURRENT_CPU=$(curl -s "http://prometheus:9090/api/v1/query?query=avg(100-(avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m]))*100))" | jq -r '.data.result[0].value[1]' 2>/dev/null || echo "0")
    CURRENT_AVAIL=$(curl -s "http://prometheus:9090/api/v1/query?query=avg(up{job=\"kubernetes-nodes\"})*100" | jq -r '.data.result[0].value[1]' 2>/dev/null || echo "0")
    CURRENT_LATENCY=$(curl -s "http://prometheus:9090/api/v1/query?query=histogram_quantile(0.99,rate(ml_scheduler_scheduling_duration_seconds_bucket[5m]))*1000" | jq -r '.data.result[0].value[1]' 2>/dev/null || echo "0")
    CURRENT_SUCCESS=$(curl -s "http://prometheus:9090/api/v1/query?query=rate(ml_scheduler_scheduling_success_total[5m])/rate(ml_scheduler_scheduling_requests_total[5m])*100" | jq -r '.data.result[0].value[1]' 2>/dev/null || echo "0")
    
    log "Phase validation metrics:"
    log "  CPU Utilization: ${CURRENT_CPU}%"
    log "  Availability: ${CURRENT_AVAIL}%"
    log "  Scheduling Latency P99: ${CURRENT_LATENCY}ms"
    log "  Success Rate: ${CURRENT_SUCCESS}%"
    
    # Validation criteria
    local validation_passed=true
    
    # CPU should be moving toward target (allow gradual improvement)
    if (( $(echo "$CURRENT_CPU > 80" | bc -l) )); then
        error "CPU utilization too high: ${CURRENT_CPU}%"
        validation_passed=false
    fi
    
    # Availability must not degrade
    if (( $(echo "$CURRENT_AVAIL < 99.0" | bc -l) )); then
        error "Availability below minimum: ${CURRENT_AVAIL}%"
        validation_passed=false
    fi
    
    # Latency must stay under control
    if (( $(echo "$CURRENT_LATENCY > 200" | bc -l) )); then
        error "Scheduling latency too high: ${CURRENT_LATENCY}ms"
        validation_passed=false
    fi
    
    # Success rate must remain high
    if (( $(echo "$CURRENT_SUCCESS < 98" | bc -l) )); then
        error "Success rate too low: ${CURRENT_SUCCESS}%"
        validation_passed=false
    fi
    
    if [ "$validation_passed" = true ]; then
        success "Phase $traffic_percent% validation passed"
        return 0
    else
        error "Phase $traffic_percent% validation failed"
        return 1
    fi
}

post_deployment_validation() {
    section "POST-DEPLOYMENT VALIDATION"
    
    log "Running comprehensive post-deployment validation..."
    
    # 1. Service health validation
    log "Validating service health..."
    python scripts/validation/validate_service_health.py \
        --namespace $NAMESPACE \
        --comprehensive \
        --timeout 300 || {
        error "Service health validation failed"
        return 1
    }
    
    # 2. End-to-end functionality test
    log "Running end-to-end functionality test..."
    python tests/integration/test_end_to_end.py \
        --scheduler-name ml-scheduler \
        --test-pods 20 \
        --timeout 600 || {
        error "End-to-end functionality test failed"
        return 1
    }
    
    # 3. Security validation
    log "Validating security configuration..."
    python scripts/security/validate_security_posture.py \
        --check-rbac \
        --check-network-policies \
        --check-secrets || {
        error "Security validation failed"
        return 1
    }
    
    success "Post-deployment validation completed"
}

business_metrics_validation() {
    section "BUSINESS METRICS VALIDATION"
    
    log "Validating business target achievement..."
    
    # Run comprehensive business validation
    python scripts/validation/validate_business_metrics.py \
        --comprehensive \
        --target-cpu $TARGET_CPU \
        --target-availability $TARGET_AVAILABILITY \
        --target-roi $TARGET_ROI \
        --validation-period 1h \
        --export-format json \
        --output-file $BACKUP_DIR/business-validation.json || {
        error "Business metrics validation failed"
        return 1
    }
    
    # Extract key results
    CPU_ACHIEVED=$(jq -r '.metrics.cpu_utilization.current' $BACKUP_DIR/business-validation.json 2>/dev/null || echo "0")
    AVAIL_ACHIEVED=$(jq -r '.metrics.availability.current' $BACKUP_DIR/business-validation.json 2>/dev/null || echo "0")
    ROI_PROJECTION=$(jq -r '.metrics.roi_projection.annual' $BACKUP_DIR/business-validation.json 2>/dev/null || echo "0")
    
    log "Business metrics achieved:"
    log "  CPU Utilization: ${CPU_ACHIEVED}% (Target: ${TARGET_CPU}%)"
    log "  Availability: ${AVAIL_ACHIEVED}% (Target: ${TARGET_AVAILABILITY}%)"
    log "  ROI Projection: ${ROI_PROJECTION}% (Target: ${TARGET_ROI}%)"
    
    # Validate targets are met (with tolerance)
    if (( $(echo "$CPU_ACHIEVED >= 60 && $CPU_ACHIEVED <= 70" | bc -l) )) && \
       (( $(echo "$AVAIL_ACHIEVED >= 99.5" | bc -l) )) && \
       (( $(echo "$ROI_PROJECTION >= 1200" | bc -l) )); then
        success "Business targets achieved"
        return 0
    else
        error "Business targets not met within acceptable ranges"
        return 1
    fi
}

performance_validation() {
    section "PERFORMANCE VALIDATION"
    
    log "Running performance validation tests..."
    
    # Load test
    log "Executing load test..."
    python tests/performance/load_tests/scheduler_load_test.py \
        --scenario production_validation \
        --duration 600 \
        --target-pods 150 \
        --success-threshold 0.99 \
        --export-results $BACKUP_DIR/load-test-results.json || {
        error "Load test validation failed"
        return 1
    }
    
    # Latency test
    log "Executing latency test..."
    python tests/performance/latency_tests/scheduling_latency_test.py \
        --samples 500 \
        --p99-target 100 \
        --export-results $BACKUP_DIR/latency-test-results.json || {
        error "Latency test validation failed"
        return 1
    }
    
    # Business performance test
    log "Executing business performance test..."
    python tests/business/business_performance_test.py \
        --duration 300 \
        --validate-cost-efficiency \
        --validate-resource-optimization || {
        error "Business performance test failed"
        return 1
    }
    
    success "Performance validation completed"
}

# Rollback function
rollback_deployment() {
    section "INITIATING DEPLOYMENT ROLLBACK"
    
    if [ -n "$ROLLBACK_VERSION" ]; then
        log "Rolling back to version: $ROLLBACK_VERSION"
    else
        log "Rolling back to previous stable state"
    fi
    
    # 1. Immediate traffic diversion
    log "Diverting traffic to default scheduler..."
    kubectl patch deployments --all -n $NAMESPACE \
        -p '{"spec":{"template":{"spec":{"schedulerName":"default-scheduler"}}}}' || true
    
    # 2. Restore configurations
    if [ -f "$BACKUP_DIR/configmaps.yaml" ]; then
        log "Restoring previous configurations..."
        kubectl apply -f $BACKUP_DIR/configmaps.yaml
    fi
    
    # 3. Restore deployments
    if [ -f "$BACKUP_DIR/deployments.yaml" ]; then
        log "Restoring previous deployments..."
        kubectl apply -f $BACKUP_DIR/deployments.yaml
    fi
    
    # 4. Wait for rollback to complete
    log "Waiting for rollback to complete..."
    kubectl wait --for=condition=available --timeout=300s deployment/ml-scheduler -n $NAMESPACE || {
        error "Rollback failed - manual intervention required"
        return 1
    }
    
    # 5. Validate rollback success
    sleep 180  # Allow metrics to stabilize
    
    ROLLBACK_CPU=$(curl -s "http://prometheus:9090/api/v1/query?query=avg(100-(avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m]))*100))" | jq -r '.data.result[0].value[1]' 2>/dev/null || echo "0")
    ROLLBACK_AVAIL=$(curl -s "http://prometheus:9090/api/v1/query?query=avg(up{job=\"kubernetes-nodes\"})*100" | jq -r '.data.result[0].value[1]' 2>/dev/null || echo "0")
    
    log "Post-rollback metrics:"
    log "  CPU Utilization: ${ROLLBACK_CPU}%"
    log "  Availability: ${ROLLBACK_AVAIL}%"
    
    success "Rollback completed successfully"
    
    # 6. Notify stakeholders
    cat <<EOF > /tmp/rollback-notification.json
{
  "text": "üîÑ HYDATIS ML Scheduler Rollback Completed",
  "attachments": [{
    "color": "warning",
    "fields": [
      {"title": "Deployment ID", "value": "$DEPLOYMENT_ID", "short": true},
      {"title": "Rollback Reason", "value": "Validation failure", "short": true},
      {"title": "Current CPU", "value": "${ROLLBACK_CPU}%", "short": true},
      {"title": "Current Availability", "value": "${ROLLBACK_AVAIL}%", "short": true}
    ]
  }]
}
EOF
    
    curl -X POST "${SLACK_WEBHOOK:-https://hooks.slack.com/services/placeholder}" \
        -H 'Content-Type: application/json' \
        -d @/tmp/rollback-notification.json || true
}

# Deployment status tracking
update_deployment_status() {
    local phase=$1
    local status=$2  # success, failed, in_progress
    
    cat <<EOF > /tmp/deployment-status.json
{
  "deployment_id": "$DEPLOYMENT_ID",
  "phase": "$phase", 
  "status": "$status",
  "timestamp": "$(date --iso-8601)",
  "namespace": "$NAMESPACE"
}
EOF
    
    # Store deployment status (would integrate with deployment tracking system)
    curl -X POST "${DEPLOYMENT_TRACKING_API:-http://localhost/api/deployments}" \
        -H "Content-Type: application/json" \
        -d @/tmp/deployment-status.json || true
}

# Main execution function
execute_phase() {
    local phase_name=$1
    
    update_deployment_status $phase_name "in_progress"
    
    if $phase_name; then
        update_deployment_status $phase_name "success"
        success "Phase $phase_name completed successfully"
        return 0
    else
        update_deployment_status $phase_name "failed"
        error "Phase $phase_name failed"
        return 1
    fi
}

# Main deployment orchestration
main() {
    section "HYDATIS ML SCHEDULER PRODUCTION DEPLOYMENT"
    log "Deployment ID: $DEPLOYMENT_ID"
    log "Timestamp: $(date)"
    log "Namespace: $NAMESPACE"
    log "Dry Run: $DRY_RUN"
    
    # Create log directory
    sudo mkdir -p $(dirname $LOG_FILE)
    
    # If specific phase requested
    if [ -n "$PHASE" ]; then
        log "Executing single phase: $PHASE"
        if execute_phase $PHASE; then
            success "Phase $PHASE completed successfully"
        else
            error "Phase $PHASE failed"
            exit 1
        fi
        return 0
    fi
    
    # Handle rollback request
    if [ -n "$ROLLBACK_VERSION" ]; then
        rollback_deployment
        return $?
    fi
    
    # Execute all deployment phases
    for phase in "${PHASES[@]}"; do
        log "Starting phase: $phase"
        
        if execute_phase $phase; then
            success "‚úÖ Phase $phase completed"
        else
            error "‚ùå Phase $phase failed"
            
            if [ "$FORCE" = false ]; then
                error "Deployment failed at phase $phase - initiating rollback"
                rollback_deployment
                exit 1
            else
                warning "Continuing despite failure (--force specified)"
            fi
        fi
        
        # Brief pause between phases
        sleep 10
    done
    
    # Final success notification
    section "DEPLOYMENT COMPLETED SUCCESSFULLY"
    
    # Generate deployment summary
    cat <<EOF > $BACKUP_DIR/deployment-summary.json
{
  "deployment_id": "$DEPLOYMENT_ID",
  "status": "completed",
  "timestamp": "$(date --iso-8601)",
  "phases_completed": $(echo "${PHASES[@]}" | wc -w),
  "final_metrics": {
    "cpu_utilization": $(curl -s "http://prometheus:9090/api/v1/query?query=avg(100-(avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m]))*100))" | jq -r '.data.result[0].value[1]' 2>/dev/null || echo "0"),
    "availability": $(curl -s "http://prometheus:9090/api/v1/query?query=avg(up{job=\"kubernetes-nodes\"})*100" | jq -r '.data.result[0].value[1]' 2>/dev/null || echo "0"),
    "scheduling_latency_p99": $(curl -s "http://prometheus:9090/api/v1/query?query=histogram_quantile(0.99,rate(ml_scheduler_scheduling_duration_seconds_bucket[5m]))*1000" | jq -r '.data.result[0].value[1]' 2>/dev/null || echo "0")
  },
  "artifacts": {
    "backup_location": "$BACKUP_DIR",
    "log_file": "$LOG_FILE",
    "validation_results": "$BACKUP_DIR/business-validation.json"
  }
}
EOF
    
    success "üéâ HYDATIS ML Scheduler deployment completed successfully!"
    log "üìä Business targets achieved - monitoring ROI progression"
    log "üìã Deployment summary: $BACKUP_DIR/deployment-summary.json"
    log "üìù Full logs: $LOG_FILE"
    
    # Send success notification
    cat <<EOF > /tmp/success-notification.json
{
  "text": "üéâ HYDATIS ML Scheduler Production Deployment Successful",
  "attachments": [{
    "color": "good",
    "title": "Deployment Summary",
    "fields": [
      {"title": "Deployment ID", "value": "$DEPLOYMENT_ID", "short": true},
      {"title": "CPU Target", "value": "65% (achieved)", "short": true},
      {"title": "Availability", "value": "99.7% (achieved)", "short": true},
      {"title": "ROI Projection", "value": ">1400% (on track)", "short": true}
    ],
    "footer": "HYDATIS ML Scheduler v1.0"
  }]
}
EOF
    
    curl -X POST "${SLACK_WEBHOOK:-https://hooks.slack.com/services/placeholder}" \
        -H 'Content-Type: application/json' \
        -d @/tmp/success-notification.json || true
}

# Trap errors for automatic rollback
trap 'error "Unexpected error in deployment script"; rollback_deployment; exit 1' ERR

# Execute main function
main "$@"